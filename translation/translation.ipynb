{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8756240,"sourceType":"datasetVersion","datasetId":5260240},{"sourceId":8759578,"sourceType":"datasetVersion","datasetId":5262761},{"sourceId":8763462,"sourceType":"datasetVersion","datasetId":5265462},{"sourceId":9110923,"sourceType":"datasetVersion","datasetId":5499006},{"sourceId":86350,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72536,"modelId":97441},{"sourceId":90412,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":75788,"modelId":100502},{"sourceId":90494,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":75855,"modelId":100566}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# installations","metadata":{"id":"3Ey--O2jcRnW"}},{"cell_type":"code","source":"# !pip install -U 'spacy[cuda-autodetect]' -q","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PgR32j24cTR9","outputId":"8bf891da-a14b-4cf0-fa6d-8df1946bbb3a","execution":{"iopub.status.busy":"2024-08-11T02:52:30.482612Z","iopub.execute_input":"2024-08-11T02:52:30.482859Z","iopub.status.idle":"2024-08-11T02:52:30.487244Z","shell.execute_reply.started":"2024-08-11T02:52:30.482835Z","shell.execute_reply":"2024-08-11T02:52:30.486416Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!python -m spacy download en_core_web_sm\n!pip install datasets\n\n# !python -m spacy download es_core_news_sm","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PWJoajxGHupW","outputId":"7b31ddee-48dd-41a7-b83b-0ad0a501ec43","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# imports","metadata":{"id":"96u-2MGiXs-4"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport math\nimport copy\nfrom functools import partial\nimport pandas as pd\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"giaE9jhGXmY-","outputId":"94e3f23e-3f4a-40e4-c0f1-aeba99d85739","execution":{"iopub.status.busy":"2024-08-11T02:52:57.658548Z","iopub.execute_input":"2024-08-11T02:52:57.659258Z","iopub.status.idle":"2024-08-11T02:53:02.766928Z","shell.execute_reply.started":"2024-08-11T02:52:57.659210Z","shell.execute_reply":"2024-08-11T02:53:02.765949Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"ydNngtERXu4H","outputId":"23cfa540-d882-4718-c633-a6741278f723","execution":{"iopub.status.busy":"2024-08-11T02:53:02.772756Z","iopub.execute_input":"2024-08-11T02:53:02.773088Z","iopub.status.idle":"2024-08-11T02:53:02.824374Z","shell.execute_reply.started":"2024-08-11T02:53:02.773055Z","shell.execute_reply":"2024-08-11T02:53:02.823290Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"random_seed = 42","metadata":{"id":"SieEu-aCLUUn","execution":{"iopub.status.busy":"2024-08-11T02:53:02.825902Z","iopub.execute_input":"2024-08-11T02:53:02.826569Z","iopub.status.idle":"2024-08-11T02:53:02.836546Z","shell.execute_reply.started":"2024-08-11T02:53:02.826541Z","shell.execute_reply":"2024-08-11T02:53:02.835544Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# multi head attention","metadata":{"id":"mJawqQ9GX8rF"}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n\n        attn_probs = torch.softmax(attn_scores, dim=-1)\n        output = torch.matmul(attn_probs, V)\n        return output\n\n    def split_heads(self, x):\n        batch_size, seq_len, d_model = x.size()\n        return x.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n\n    def combine_heads(self, x):\n        batch_size, _, seq_len, d_k = x.size()\n        return x.transpose(1, 2).reshape(batch_size, seq_len, self.d_model)\n\n    def forward(self, Q, K, V, mask=None):\n        Q = self.split_heads(self.W_q(Q))\n        K = self.split_heads(self.W_k(K))\n        V = self.split_heads(self.W_v(V))\n\n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        output = self.W_o(self.combine_heads(attn_output))\n        return output\n","metadata":{"id":"CnGEAqsBXzkU","execution":{"iopub.status.busy":"2024-08-11T02:53:02.837793Z","iopub.execute_input":"2024-08-11T02:53:02.838051Z","iopub.status.idle":"2024-08-11T02:53:02.850518Z","shell.execute_reply.started":"2024-08-11T02:53:02.838028Z","shell.execute_reply":"2024-08-11T02:53:02.849665Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# PositionWiseFeedForward","metadata":{"id":"L7yfVhkMYEhx"}},{"cell_type":"code","source":"class PositionWiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(PositionWiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))","metadata":{"id":"nhLEdRqBYAyN","execution":{"iopub.status.busy":"2024-08-11T02:53:02.851632Z","iopub.execute_input":"2024-08-11T02:53:02.852220Z","iopub.status.idle":"2024-08-11T02:53:02.870047Z","shell.execute_reply.started":"2024-08-11T02:53:02.852190Z","shell.execute_reply":"2024-08-11T02:53:02.869150Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# positional encoding","metadata":{"id":"nv9loWVOYKQt"}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_length):\n        super(PositionalEncoding, self).__init__()\n\n        pe = torch.zeros(max_seq_length, d_model, device=device)\n        position = torch.arange(0, max_seq_length, dtype=torch.float, device=device).unsqueeze(1)\n        div_term = torch.pow(10_000, (-torch.arange(0, d_model, 2, device=device).float() / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        return self.register_buffer('pe', pe.unsqueeze(0))\n\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]","metadata":{"id":"_SqsjrSaYIlt","execution":{"iopub.status.busy":"2024-08-11T02:53:02.873044Z","iopub.execute_input":"2024-08-11T02:53:02.873422Z","iopub.status.idle":"2024-08-11T02:53:02.886352Z","shell.execute_reply.started":"2024-08-11T02:53:02.873376Z","shell.execute_reply":"2024-08-11T02:53:02.885433Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# encoder","metadata":{"id":"ZKUrDvlYYRWk"}},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(EncoderLayer, self).__init__()\n\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        return x\n","metadata":{"id":"i-Suvxv0YOqh","execution":{"iopub.status.busy":"2024-08-11T02:53:02.887389Z","iopub.execute_input":"2024-08-11T02:53:02.887682Z","iopub.status.idle":"2024-08-11T02:53:02.904449Z","shell.execute_reply.started":"2024-08-11T02:53:02.887659Z","shell.execute_reply":"2024-08-11T02:53:02.903430Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# decoder layer","metadata":{"id":"Z8ySRXYzYYH9"}},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(DecoderLayer, self).__init__()\n\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n\n    def forward(self, x, enc_output, src_mask, tgt_mask):\n        attn_output = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n        return x","metadata":{"id":"HRTalpKJYU6P","execution":{"iopub.status.busy":"2024-08-11T02:53:02.909306Z","iopub.execute_input":"2024-08-11T02:53:02.909702Z","iopub.status.idle":"2024-08-11T02:53:02.918468Z","shell.execute_reply.started":"2024-08-11T02:53:02.909677Z","shell.execute_reply":"2024-08-11T02:53:02.917529Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# transformer","metadata":{"id":"YubbLshNYekk"}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n        super(Transformer, self).__init__()\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n\n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n\n        self.fc = nn.Linear(d_model, tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def generate_mask(self, src, tgt):\n        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n        seq_length = tgt.size(1)\n        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=device), diagonal=1)).bool()\n        tgt_mask = tgt_mask & nopeak_mask\n        return src_mask, tgt_mask\n\n    def forward(self, src, tgt):\n        src_mask, tgt_mask = self.generate_mask(src, tgt)\n\n        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n\n        enc_output = src_embedded\n        for enc_layer in self.encoder_layers:\n            enc_output = enc_layer(enc_output, src_mask)\n\n        dec_output = tgt_embedded\n        for dec_layer in self.decoder_layers:\n            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n\n        output = self.fc(dec_output)\n        return output","metadata":{"id":"x6RA6zpgYbHF","execution":{"iopub.status.busy":"2024-08-11T02:53:02.919765Z","iopub.execute_input":"2024-08-11T02:53:02.920074Z","iopub.status.idle":"2024-08-11T02:53:02.942104Z","shell.execute_reply.started":"2024-08-11T02:53:02.920045Z","shell.execute_reply":"2024-08-11T02:53:02.941239Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# data","metadata":{"id":"5Fu_f8PgYnXw"}},{"cell_type":"markdown","source":"## change these paths","metadata":{}},{"cell_type":"code","source":"lines_en = open('/kaggle/input/translation-data-full/train.en.txt', encoding='utf-8').\\\n        read().strip().split('\\n')\nlines_vi = open('/kaggle/input/translation-data-full/train.vi.txt', encoding='utf-8').\\\n        read().strip().split('\\n')\n\nlines_en_val = open('/kaggle/input/translation-data-full/val.en.txt', encoding='utf-8').\\\n        read().strip().split('\\n')\nlines_vi_val = open('/kaggle/input/translation-data-full/val.vi.txt', encoding='utf-8').\\\n        read().strip().split('\\n')\n\nlines_en_test = open('/kaggle/input/translation-data-full/test.en.txt', encoding='utf-8').\\\n        read().strip().split('\\n')\nlines_vi_test = open('/kaggle/input/translation-data-full/test.vi.txt', encoding='utf-8').\\\n        read().strip().split('\\n')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T02:53:02.943203Z","iopub.execute_input":"2024-08-11T02:53:02.943552Z","iopub.status.idle":"2024-08-11T02:53:03.705344Z","shell.execute_reply.started":"2024-08-11T02:53:02.943522Z","shell.execute_reply":"2024-08-11T02:53:03.704380Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"lines_train = []\nlines_val = []\nlines_test = []\n\nfor i in range(len(lines_en)):\n    try:\n        lmao = '\\t'.join([lines_en[i], lines_vi[i]])\n        lines_train.append(lmao)\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        continue\nfor i in range(len(lines_en_val)):\n    try:\n        lmao = '\\t'.join([lines_en_val[i], lines_vi_val[i]])\n        lines_val.append(lmao)\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        continue\nfor i in range(len(lines_en_val)):\n    try:\n        lmao = '\\t'.join([lines_en_test[i], lines_vi_test[i]])\n        lines_test.append(lmao)\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        continue\n\ntrain_lines = lines_train\nval_lines = lines_val\ntest_lines = lines_test","metadata":{"id":"OUCSX2QzaEJM","execution":{"iopub.status.busy":"2024-08-11T02:53:03.706784Z","iopub.execute_input":"2024-08-11T02:53:03.707458Z","iopub.status.idle":"2024-08-11T02:53:03.851966Z","shell.execute_reply.started":"2024-08-11T02:53:03.707420Z","shell.execute_reply":"2024-08-11T02:53:03.850858Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import re\ndef clean_text(sentence):\n    sentence = re.sub(r\" &quot;\", \"\\\"\", sentence)  # Handle &quot;\n    sentence = re.sub(r\" &apos;\", \"'\", sentence)  # Handle &apos;\n    sentence = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n    sentence = re.sub(r\"[ ]+\", \" \", sentence)\n    sentence = re.sub(r\"\\!+\", \"!\", sentence)\n    sentence = re.sub(r\"\\,+\", \",\", sentence)\n    sentence = re.sub(r\"\\?+\", \"?\", sentence)\n\n    # Lowercase the sentence\n    sentence = sentence.lower()\n\n    return sentence","metadata":{"id":"CptVXMmfS0Tl","execution":{"iopub.status.busy":"2024-08-11T02:53:03.875815Z","iopub.execute_input":"2024-08-11T02:53:03.876138Z","iopub.status.idle":"2024-08-11T02:53:03.884137Z","shell.execute_reply.started":"2024-08-11T02:53:03.876109Z","shell.execute_reply":"2024-08-11T02:53:03.883257Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_lines = [clean_text(line) for line in train_lines]\nval_lines = [clean_text(line) for line in val_lines]\ntest_lines = [clean_text(line) for line in test_lines]","metadata":{"id":"jAVowfqr-cu9","execution":{"iopub.status.busy":"2024-08-11T02:53:03.885871Z","iopub.execute_input":"2024-08-11T02:53:03.886188Z","iopub.status.idle":"2024-08-11T02:53:09.539117Z","shell.execute_reply.started":"2024-08-11T02:53:03.886158Z","shell.execute_reply":"2024-08-11T02:53:09.538338Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"test_lines[:5]","metadata":{"id":"ro8IoQC1TPUk","execution":{"iopub.status.busy":"2024-08-11T02:53:09.540201Z","iopub.execute_input":"2024-08-11T02:53:09.540550Z","iopub.status.idle":"2024-08-11T02:53:09.546385Z","shell.execute_reply.started":"2024-08-11T02:53:09.540525Z","shell.execute_reply":"2024-08-11T02:53:09.545464Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"['borders between the bosniak territory and the bosnian serb territory have calmed down .\\tbiên giới giữa lãnh thỗ người bosnia theo đạo hồi và lãnh thổ người bosnia serbi đã bình ổn .',\n 'we created the responsive room where the lights music and blinds adjusted to your state .\\tchúng tôi đã tạo ra một phòng cảm ứng trong đó ánh sáng , nhạc và rèm cửa được tuỳ chỉnh theo trạng thái của bạn .',\n 'we are seeing the rise of female sexual expression .\\tchúng ta đang thấy sự trỗi dậy trong việc biểu lộ giới tính của phụ nữ .',\n 'now , the first of these transformations is going to happen anyway .\\thiện giờ , cuộc biến đổi đầu tiên rốt cuộc cũng sắp sửa xảy ra .',\n 'do you know what it is ? anyone ?\\tcác bạn có biết là gì không ? có ai biết không ?']"},"metadata":{}}]},{"cell_type":"markdown","source":"# Preprocess Data","metadata":{"id":"EA5cl9qOIukd"}},{"cell_type":"code","source":"SRC_LANGUAGE = \"en\"\nTGT_LANGUAGE = \"vi\"","metadata":{"id":"XWO6oUP9aQrE","execution":{"iopub.status.busy":"2024-08-11T02:53:09.547668Z","iopub.execute_input":"2024-08-11T02:53:09.548017Z","iopub.status.idle":"2024-08-11T02:53:09.557057Z","shell.execute_reply.started":"2024-08-11T02:53:09.547984Z","shell.execute_reply":"2024-08-11T02:53:09.556050Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"tokenizer = {}\ntokenizer[SRC_LANGUAGE] = get_tokenizer(\"spacy\", \"en_core_web_sm\")\ntokenizer[TGT_LANGUAGE] = get_tokenizer(\"spacy\", \"en_core_web_sm\")","metadata":{"id":"K57FvxR9HjHS","execution":{"iopub.status.busy":"2024-08-11T02:53:09.558287Z","iopub.execute_input":"2024-08-11T02:53:09.558901Z","iopub.status.idle":"2024-08-11T02:53:13.361210Z","shell.execute_reply.started":"2024-08-11T02:53:09.558867Z","shell.execute_reply":"2024-08-11T02:53:13.360448Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## make dataset","metadata":{"id":"av5y5eatMm3P"}},{"cell_type":"code","source":"class SentencePairDataset(Dataset):\n    def __init__(self, lines, src_tokenizer, tgt_tokenizer):\n        super(SentencePairDataset, self).__init__()\n\n        self.lines = lines\n        self.src_tokenizer = src_tokenizer\n        self.tgt_tokenizer = tgt_tokenizer\n\n    def __len__(self):\n        return len(self.lines)\n\n    def __getitem__(self, idx):\n        line = self.lines[idx]\n\n        src, tgt = line.split('\\t')\n        src_tokens = self.src_tokenizer(src)\n        tgt_tokens = self.tgt_tokenizer(tgt)\n\n        return src_tokens, tgt_tokens","metadata":{"id":"9pQb9TXQIQHU","execution":{"iopub.status.busy":"2024-08-11T02:53:13.362417Z","iopub.execute_input":"2024-08-11T02:53:13.363298Z","iopub.status.idle":"2024-08-11T02:53:13.370366Z","shell.execute_reply.started":"2024-08-11T02:53:13.363263Z","shell.execute_reply":"2024-08-11T02:53:13.369454Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_ds = SentencePairDataset(train_lines, tokenizer[SRC_LANGUAGE], tokenizer[TGT_LANGUAGE])\nval_ds = SentencePairDataset(val_lines, tokenizer[SRC_LANGUAGE], tokenizer[TGT_LANGUAGE])\ntest_ds = SentencePairDataset(test_lines, tokenizer[SRC_LANGUAGE], tokenizer[TGT_LANGUAGE])","metadata":{"id":"_VhZbqdSKIn5","execution":{"iopub.status.busy":"2024-08-11T02:53:13.371560Z","iopub.execute_input":"2024-08-11T02:53:13.371889Z","iopub.status.idle":"2024-08-11T02:53:13.385276Z","shell.execute_reply.started":"2024-08-11T02:53:13.371855Z","shell.execute_reply":"2024-08-11T02:53:13.384435Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## vocabulary","metadata":{"id":"Yqo3q9RkMkJC"}},{"cell_type":"code","source":"vocab = {}","metadata":{"id":"rvyraKQ4r9gG","execution":{"iopub.status.busy":"2024-08-11T02:53:13.386284Z","iopub.execute_input":"2024-08-11T02:53:13.386550Z","iopub.status.idle":"2024-08-11T02:53:13.398238Z","shell.execute_reply.started":"2024-08-11T02:53:13.386528Z","shell.execute_reply":"2024-08-11T02:53:13.397562Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"src_vocab_size = 10_000\ntgt_vocab_size = 10_000\nmax_seq_len = 100\n\nPAD_IDX = 0\nUNK_IDX = 1\nBOS_IDX = 2\nEOS_IDX = 3\n\nspecial_symbols = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']","metadata":{"id":"f4FMeg4yMP8x","execution":{"iopub.status.busy":"2024-08-11T02:53:13.399166Z","iopub.execute_input":"2024-08-11T02:53:13.399447Z","iopub.status.idle":"2024-08-11T02:53:13.411081Z","shell.execute_reply.started":"2024-08-11T02:53:13.399415Z","shell.execute_reply":"2024-08-11T02:53:13.410212Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def yield_tokens(dataset, lang_idx=0):\n    n = len(dataset)\n    i = 0\n\n    while i < n:\n        yield dataset[i][lang_idx]\n        i += 1","metadata":{"id":"ujR7SLRvM1h7","execution":{"iopub.status.busy":"2024-08-11T02:53:13.412083Z","iopub.execute_input":"2024-08-11T02:53:13.412428Z","iopub.status.idle":"2024-08-11T02:53:13.422570Z","shell.execute_reply.started":"2024-08-11T02:53:13.412379Z","shell.execute_reply":"2024-08-11T02:53:13.421729Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"src_iterator = yield_tokens(train_ds, lang_idx=0)\ntgt_iterator = yield_tokens(train_ds, lang_idx=1)","metadata":{"id":"knnnCTpMNGMA","execution":{"iopub.status.busy":"2024-08-11T02:53:13.423511Z","iopub.execute_input":"2024-08-11T02:53:13.423761Z","iopub.status.idle":"2024-08-11T02:53:13.439420Z","shell.execute_reply.started":"2024-08-11T02:53:13.423731Z","shell.execute_reply":"2024-08-11T02:53:13.438534Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"vocab[SRC_LANGUAGE] = build_vocab_from_iterator(\n    src_iterator,\n    min_freq=1,\n    specials=special_symbols,\n    special_first=True,\n    max_tokens=src_vocab_size,\n)\nvocab[TGT_LANGUAGE] = build_vocab_from_iterator(\n    tgt_iterator,\n    min_freq=1,\n    specials=special_symbols,\n    special_first=True,\n    max_tokens=tgt_vocab_size,\n)","metadata":{"id":"igmo37DaNTox","execution":{"iopub.status.busy":"2024-08-11T02:53:13.440484Z","iopub.execute_input":"2024-08-11T02:53:13.440748Z","iopub.status.idle":"2024-08-11T02:53:44.809919Z","shell.execute_reply.started":"2024-08-11T02:53:13.440727Z","shell.execute_reply":"2024-08-11T02:53:44.808999Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"vocab[SRC_LANGUAGE].set_default_index(UNK_IDX)\nvocab[TGT_LANGUAGE].set_default_index(UNK_IDX)","metadata":{"id":"J5I7rHfFOMTP","execution":{"iopub.status.busy":"2024-08-11T02:53:44.816017Z","iopub.execute_input":"2024-08-11T02:53:44.816321Z","iopub.status.idle":"2024-08-11T02:53:44.822649Z","shell.execute_reply.started":"2024-08-11T02:53:44.816296Z","shell.execute_reply":"2024-08-11T02:53:44.821629Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch, vocab):\n    batch_size = len(batch)\n    srcs, tgts = zip(*batch)\n    src_vectors = torch.zeros((batch_size, max_seq_len), dtype=torch.long, device=device)\n    tgt_vectors = torch.zeros((batch_size, max_seq_len), dtype=torch.long, device=device)\n\n    for i in range(batch_size):\n        src_vectors[i] = torch.tensor(([BOS_IDX] + vocab[SRC_LANGUAGE](srcs[i]) + [EOS_IDX] + [0] * (max_seq_len - len(srcs[i])))[:max_seq_len], dtype=torch.long, device=device)\n        tgt_vectors[i] = torch.tensor(([BOS_IDX] + vocab[TGT_LANGUAGE](tgts[i]) + [EOS_IDX] + [0] * (max_seq_len - len(tgts[i])))[:max_seq_len], dtype=torch.long, device=device)\n\n    return src_vectors, tgt_vectors","metadata":{"id":"hfK2UQZFmxQD","execution":{"iopub.status.busy":"2024-08-11T02:53:44.823702Z","iopub.execute_input":"2024-08-11T02:53:44.824045Z","iopub.status.idle":"2024-08-11T02:53:44.839573Z","shell.execute_reply.started":"2024-08-11T02:53:44.824007Z","shell.execute_reply":"2024-08-11T02:53:44.838717Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=partial(collate_fn, vocab=vocab))\nval_dataloader = DataLoader(val_ds, batch_size=64, shuffle=True, collate_fn=partial(collate_fn, vocab=vocab))\ntest_dataloader = DataLoader(test_ds, batch_size=64, shuffle=True, collate_fn=partial(collate_fn, vocab=vocab))","metadata":{"id":"Uqxlzw-Hu_YJ","execution":{"iopub.status.busy":"2024-08-11T02:53:44.840694Z","iopub.execute_input":"2024-08-11T02:53:44.841019Z","iopub.status.idle":"2024-08-11T02:53:44.852713Z","shell.execute_reply.started":"2024-08-11T02:53:44.840986Z","shell.execute_reply":"2024-08-11T02:53:44.852017Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# train","metadata":{"id":"c7Zi-kGsQMqN"}},{"cell_type":"code","source":"src_vocab_size = 10000\ntgt_vocab_size = 10000\nd_model = 512\nnum_heads = 4\nnum_layers = 6\nd_ff = 2048\nmax_seq_length = 100\ndropout = 0.1\n\ntransformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T02:53:44.853895Z","iopub.execute_input":"2024-08-11T02:53:44.854511Z","iopub.status.idle":"2024-08-11T02:53:47.414460Z","shell.execute_reply.started":"2024-08-11T02:53:44.854479Z","shell.execute_reply":"2024-08-11T02:53:47.413679Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"num_epochs = 5\n\ntransformer.train()\n\nfor epoch in range(num_epochs):\n    print(f\"Epoch: {epoch+1}\\n------------------------------\")\n    transformer.train()\n    for data in train_dataloader:\n        src_data, tgt_data = data\n        optimizer.zero_grad()\n        output = transformer(src_data, tgt_data[:, :-1])\n        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n        loss.backward()\n        optimizer.step()\n        print(f\"Epoch: {epoch+1}, Training Loss: {loss.item()}\")\n\n    transformer.eval()\n    with torch.no_grad():\n        for data in val_dataloader:\n            src_data, tgt_data = data\n            output = transformer(src_data, tgt_data[:, :-1])\n            loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n            print(f\"Epoch: {epoch+1}, Validation Loss: {loss.item()}\")\n\n    torch.save(transformer.state_dict(), f'./transformer_state_dict_epoch_{epoch+1}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gxUsDY5JzNj7","outputId":"4e4e95bc-104b-45f0-e8e2-ca87b15bc4ca","_kg_hide-output":false,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer.eval()\nwith torch.no_grad():\n    for data in val_dataloader:\n        src_data, tgt_data = data\n        output = transformer(src_data, tgt_data[:, :-1])\n        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n        print(f\"Test Loss: {loss.item()}\")","metadata":{"id":"qLQfPbuV_MW4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer.eval()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T04:18:53.566763Z","iopub.execute_input":"2024-08-11T04:18:53.567375Z","iopub.status.idle":"2024-08-11T04:18:53.576236Z","shell.execute_reply.started":"2024-08-11T04:18:53.567339Z","shell.execute_reply":"2024-08-11T04:18:53.575343Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"Transformer(\n  (encoder_embedding): Embedding(10000, 512)\n  (decoder_embedding): Embedding(10000, 512)\n  (positional_encoding): PositionalEncoding()\n  (encoder_layers): ModuleList(\n    (0-5): 6 x EncoderLayer(\n      (self_attn): MultiHeadAttention(\n        (W_q): Linear(in_features=512, out_features=512, bias=True)\n        (W_k): Linear(in_features=512, out_features=512, bias=True)\n        (W_v): Linear(in_features=512, out_features=512, bias=True)\n        (W_o): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (feed_forward): PositionWiseFeedForward(\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n        (relu): ReLU()\n      )\n      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (decoder_layers): ModuleList(\n    (0-5): 6 x DecoderLayer(\n      (self_attn): MultiHeadAttention(\n        (W_q): Linear(in_features=512, out_features=512, bias=True)\n        (W_k): Linear(in_features=512, out_features=512, bias=True)\n        (W_v): Linear(in_features=512, out_features=512, bias=True)\n        (W_o): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (cross_attn): MultiHeadAttention(\n        (W_q): Linear(in_features=512, out_features=512, bias=True)\n        (W_k): Linear(in_features=512, out_features=512, bias=True)\n        (W_v): Linear(in_features=512, out_features=512, bias=True)\n        (W_o): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (feed_forward): PositionWiseFeedForward(\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n        (relu): ReLU()\n      )\n      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (fc): Linear(in_features=512, out_features=10000, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def translate(src):\n    src_tokens = tokenizer[SRC_LANGUAGE](src)\n    tgt_tokens = [\"<BOS>\"]\n\n    src_vectors = torch.tensor(([BOS_IDX] + vocab[SRC_LANGUAGE](src_tokens) + [EOS_IDX] + [0] * (max_seq_len - len(src_tokens)))[:max_seq_len], dtype=torch.long, device=device).unsqueeze(0)\n\n    for i in range(max_seq_len):\n        tgt_vectors = torch.tensor((vocab[TGT_LANGUAGE](tgt_tokens) + [0] * (max_seq_len - len(tgt_tokens)))[:max_seq_len], dtype=torch.long, device=device).unsqueeze(0)\n        output = transformer(src_vectors, tgt_vectors)\n        idx = torch.argmax(nn.functional.softmax(output, dim=2)[0][i]).item()\n        tgt_tokens.append(vocab[TGT_LANGUAGE].lookup_token(idx))\n\n        if idx == EOS_IDX:\n            break\n\n    return \" \".join(tgt_tokens).replace(\"<BOS>\", \"\").replace(\"<EOS>\", \"\").replace(\"<PAD>\", \"\").strip()","metadata":{"id":"r1fxXSdbHGq_","execution":{"iopub.status.busy":"2024-08-11T04:18:53.577440Z","iopub.execute_input":"2024-08-11T04:18:53.578144Z","iopub.status.idle":"2024-08-11T04:18:53.597584Z","shell.execute_reply.started":"2024-08-11T04:18:53.578111Z","shell.execute_reply":"2024-08-11T04:18:53.596696Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"translate(\"i love you\")","metadata":{"execution":{"iopub.status.busy":"2024-08-11T04:18:53.598663Z","iopub.execute_input":"2024-08-11T04:18:53.598985Z","iopub.status.idle":"2024-08-11T04:18:53.741303Z","shell.execute_reply.started":"2024-08-11T04:18:53.598961Z","shell.execute_reply":"2024-08-11T04:18:53.740447Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"'tôi yêu quý vị'"},"metadata":{}}]},{"cell_type":"code","source":"import nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nsmoother = SmoothingFunction()\ndef compute_bleu_score(transformer, src, ref_tgt, tokenizer, vocab, max_seq_len, device):\n    generated_translation = translate(src)\n\n    ref_tokens = tokenizer[TGT_LANGUAGE](ref_tgt)\n    bleu_score = sentence_bleu([ref_tokens], generated_translation.split())\n    return bleu_score\n","metadata":{"id":"yABn4joXtyX5","execution":{"iopub.status.busy":"2024-08-11T04:18:53.764310Z","iopub.execute_input":"2024-08-11T04:18:53.764613Z","iopub.status.idle":"2024-08-11T04:18:54.793985Z","shell.execute_reply.started":"2024-08-11T04:18:53.764591Z","shell.execute_reply":"2024-08-11T04:18:54.793125Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# Calculate BLEU","metadata":{}},{"cell_type":"code","source":"def compute_bleu_for_dataset(transformer, lines_en_val, lines_vi_val, tokenizer, vocab, max_seq_len, device):\n    total_bleu_score = 0.0\n    num_examples = 500\n\n    for i in range(num_examples):\n        if i % 25 == 0:\n            print(f'currently at sentence number {i} out of {num_examples}')\n        src_sentence = lines_en_val[i]\n        ref_sentence = lines_vi_val[i]\n        ref_sentence = clean_text(ref_sentence)\n#         print(f'src_sentence {i}: ', src_sentence)\n#         print(f'ref_sentence {i}: ', ref_sentence)\n        bleu_score = compute_bleu_score(transformer, src_sentence, ref_sentence, tokenizer, vocab, max_seq_len, device)\n#         print(f'bleu_score {i}: ', bleu_score)\n        total_bleu_score += bleu_score\n\n    average_bleu_score = total_bleu_score * 100/ num_examples\n#     print(total_bleu_score)\n#     print(num_examples)\n    return average_bleu_score\n\naverage_bleu = compute_bleu_for_dataset(transformer, lines_en_val, lines_vi_val, tokenizer, vocab, max_seq_len, device)\nprint(f\"Average BLEU Score on Validation Dataset: {average_bleu}\")","metadata":{"id":"Bx2fnZfm3Hl-","scrolled":true,"execution":{"iopub.status.busy":"2024-08-11T04:18:54.795091Z","iopub.execute_input":"2024-08-11T04:18:54.795348Z","iopub.status.idle":"2024-08-11T04:21:51.482547Z","shell.execute_reply.started":"2024-08-11T04:18:54.795325Z","shell.execute_reply":"2024-08-11T04:21:51.481618Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"currently at sentence number 0 out of 500\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 4-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"currently at sentence number 25 out of 500\ncurrently at sentence number 50 out of 500\ncurrently at sentence number 75 out of 500\ncurrently at sentence number 100 out of 500\ncurrently at sentence number 125 out of 500\ncurrently at sentence number 150 out of 500\ncurrently at sentence number 175 out of 500\ncurrently at sentence number 200 out of 500\ncurrently at sentence number 225 out of 500\ncurrently at sentence number 250 out of 500\ncurrently at sentence number 275 out of 500\ncurrently at sentence number 300 out of 500\ncurrently at sentence number 325 out of 500\ncurrently at sentence number 350 out of 500\ncurrently at sentence number 375 out of 500\ncurrently at sentence number 400 out of 500\ncurrently at sentence number 425 out of 500\ncurrently at sentence number 450 out of 500\ncurrently at sentence number 475 out of 500\nAverage BLEU Score on Validation Dataset: 31.697098993675464\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Average BLEU Score on Validation Dataset: 31.697098993675464","metadata":{}},{"cell_type":"code","source":"torch.save(transformer, 'transformer_translation.pth')","metadata":{"id":"y57exdnTjQDP","execution":{"iopub.status.busy":"2024-08-11T05:52:33.737953Z","iopub.execute_input":"2024-08-11T05:52:33.738349Z","iopub.status.idle":"2024-08-11T05:52:34.093073Z","shell.execute_reply.started":"2024-08-11T05:52:33.738318Z","shell.execute_reply":"2024-08-11T05:52:34.092304Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"# Inference ","metadata":{}},{"cell_type":"markdown","source":"## config the path to model .pth file","metadata":{}},{"cell_type":"code","source":"# change this path\ntransformer = torch.load('/kaggle/working/transformer_translation.pth')\ntransformer.eval()  ","metadata":{"execution":{"iopub.status.busy":"2024-08-11T05:27:16.017635Z","iopub.status.idle":"2024-08-11T05:27:16.017974Z","shell.execute_reply.started":"2024-08-11T05:27:16.017815Z","shell.execute_reply":"2024-08-11T05:27:16.017829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tune","metadata":{}},{"cell_type":"markdown","source":"## config the path to model .pth file","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport pandas as pd\nimport transformers\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import AutoModelForSeq2SeqLM\n\n\nfrom datasets import Dataset, DatasetDict\nfrom sklearn.model_selection import train_test_split\nmodel_checkpoint = \"Helsinki-NLP/opus-mt-en-vi\"\nmodel_finetune = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\ntokenizer_finetune = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n\n# change this path\nmodel_finetune.load_state_dict(torch.load('/kaggle/input/fine_tune/pytorch/default/1/6_8_24_model_finetune_translation.pth'))\nmodel_finetune.eval()","metadata":{"execution":{"iopub.status.busy":"2024-08-11T05:31:48.510428Z","iopub.execute_input":"2024-08-11T05:31:48.511287Z","iopub.status.idle":"2024-08-11T05:31:51.430576Z","shell.execute_reply.started":"2024-08-11T05:31:48.511252Z","shell.execute_reply":"2024-08-11T05:31:51.429630Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"MarianMTModel(\n  (model): MarianModel(\n    (shared): Embedding(53685, 512, padding_idx=53684)\n    (encoder): MarianEncoder(\n      (embed_tokens): Embedding(53685, 512, padding_idx=53684)\n      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n      (layers): ModuleList(\n        (0-5): 6 x MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLU()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (decoder): MarianDecoder(\n      (embed_tokens): Embedding(53685, 512, padding_idx=53684)\n      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n      (layers): ModuleList(\n        (0-5): 6 x MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLU()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=512, out_features=53685, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def translate_finetune_model(input_text):\n    # input_text = \"My mom once told me that as long as we put our mind into our work, we will succeed\"\n    inputs = tokenizer_finetune(input_text, return_tensors=\"pt\")\n    outputs = model_finetune.generate(\n        inputs[\"input_ids\"],\n        max_length=50,  # Adjust max_length as needed\n        num_beams=5,    # Adjust num_beams for beam search\n        early_stopping=True\n    )\n    translated_text = tokenizer_finetune.decode(outputs[0], skip_special_tokens=True)\n    \n    return translated_text","metadata":{"execution":{"iopub.status.busy":"2024-08-11T05:48:43.637986Z","iopub.execute_input":"2024-08-11T05:48:43.638767Z","iopub.status.idle":"2024-08-11T05:48:43.644444Z","shell.execute_reply.started":"2024-08-11T05:48:43.638729Z","shell.execute_reply":"2024-08-11T05:48:43.643415Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"def translate_from_scratch_model(src):\n    return translate(src)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T05:48:45.256480Z","iopub.execute_input":"2024-08-11T05:48:45.256836Z","iopub.status.idle":"2024-08-11T05:48:45.261241Z","shell.execute_reply.started":"2024-08-11T05:48:45.256808Z","shell.execute_reply":"2024-08-11T05:48:45.260320Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"!pip install gradio","metadata":{"execution":{"iopub.status.busy":"2024-08-11T05:27:16.026255Z","iopub.status.idle":"2024-08-11T05:27:16.026801Z","shell.execute_reply.started":"2024-08-11T05:27:16.026570Z","shell.execute_reply":"2024-08-11T05:27:16.026589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gradio as gr\n\n# Example sentences used for translation\nexample_sentences = [\n    \"What is the weather like today?\",\n    \"Hello everyone, this is our school, we are happy to have you here.\",\n    \"How was your first day of school?\",\n]\ntranslation_history = []\n\ndef translate_text(input_text):\n    # Placeholder function to simulate translation\n    translated_text_model_1 = translate_from_scratch_model(input_text)\n    translated_text_model_2 = translate_finetune_model(input_text)\n\n    # Save translation history\n    translation_history.append({\n        \"original_text\": input_text,\n        \"model_1_translation\": translated_text_model_1,\n        \"model_2_translation\": translated_text_model_2,\n    })\n\n    # Format history for display\n    formatted_history = [\n        [\n            entry[\"original_text\"],\n            entry[\"model_1_translation\"],\n            entry[\"model_2_translation\"]\n        ] for entry in translation_history\n    ]\n\n    return translated_text_model_1, translated_text_model_2, formatted_history\n\ndef interface():\n    with gr.Blocks() as demo:\n        gr.Markdown(\"## Translation Interface\")\n\n        with gr.Row():\n            with gr.Column():\n                text_input = gr.Textbox(label=\"Enter text to translate\")\n                example_sentences_box = gr.Textbox(label=\"Example Sentences\", value=\"\\n\".join(example_sentences), interactive=False)\n                translate_button = gr.Button(\"Translate\")\n            with gr.Column():\n                model_1_output = gr.Textbox(label=\"Transformer from scratch Translation\", interactive=False)\n                model_2_output = gr.Textbox(label=\"Finetune Translation\", interactive=False)\n                history_output = gr.Dataframe(headers=[\"History translation\", \"Transformer from scratch Translation\", \"Finetune Translation\"], row_count=5)\n\n        translate_button.click(translate_text, inputs=text_input, outputs=[model_1_output, model_2_output, history_output])\n\n    return demo\n\nif __name__ == \"__main__\":\n    demo = interface()\n    demo.launch()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T05:27:16.028196Z","iopub.status.idle":"2024-08-11T05:27:16.028657Z","shell.execute_reply.started":"2024-08-11T05:27:16.028430Z","shell.execute_reply":"2024-08-11T05:27:16.028449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}